# stochastic-gradient-descent

Stochastic gradient descent is an iterative method for optimizing an objective function with suitable smoothness properties. The idea behind stochastic approximation can be traced back to the Robbinsâ€“Monro algorithm of the 1950s, and it has become become an important optimization method in machine learning.
